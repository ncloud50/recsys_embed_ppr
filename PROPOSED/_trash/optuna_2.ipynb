{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from networkx.exception import NetworkXError\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "\n",
    "from dataloader import AmazonDataset\n",
    "import models\n",
    "from models import DistMulti, TransE\n",
    "from training import TrainIterater\n",
    "from evaluate import Evaluater\n",
    "\n",
    "import optuna\n",
    "import time \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'TransE'\n",
    "dataset = AmazonDataset('./data', model_name='TransE')\n",
    "edges = [[r[0], r[1]] for r in dataset.triplet_df.values]\n",
    "\n",
    "# ハイパラ\n",
    "best_params = pickle.load(open('./best_param.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KG embedする\n",
    "\n",
    "bestなハイパラパラメータを読み込んでepoch回す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_embed(params):\n",
    "    \n",
    "    # ハイパラ読み込み\n",
    "    embedding_dim = best_params['embedding_dim']\n",
    "    batch_size = best_params['batch_size']\n",
    "    lr = best_params['lr']\n",
    "    weight_decay = best_params['weight_decay']\n",
    "    warmup = best_params['warmup']\n",
    "    lr_decay_every = best_params['lr_decay_every']\n",
    "    lr_decay_rate = best_params['lr_decay_rate']\n",
    "    \n",
    "    relation_size = len(set(list(dataset.triplet_df['relation'].values)))\n",
    "    entity_size = len(dataset.entity_list)\n",
    "    model = TransE(int(embedding_dim), relation_size, entity_size).to(device)\n",
    "    iterater = TrainIterater(batch_size=int(batch_size), model_name=model_name)\n",
    "    score =iterater.iterate_epoch(model, lr=lr, epoch=5000, weight_decay=weight_decay, warmup=warmup,\n",
    "                           lr_decay_rate=lr_decay_rate, lr_decay_every=lr_decay_every, eval_every=1e+5)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# とりあえず初期化したモデルのembeddingを使って進める"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 16\n",
    "relation_size = len(set(list(dataset.triplet_df['relation'].values)))\n",
    "entity_size = len(dataset.entity_list)\n",
    "embed_model = TransE(int(embedding_dim), relation_size, entity_size).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph()\n",
    "G.add_nodes_from([i for i in range(len(dataset.entity_list))])\n",
    "G.add_edges_from(edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sparse sim_matを作る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_sparse_sim_mat(model):\n",
    "    item_idx = torch.tensor([dataset.entity_list.index(i) for i in dataset.item_list], \n",
    "                        dtype=torch.long, device=device)\n",
    "\n",
    "    user_idx = torch.tensor([dataset.entity_list.index(u) for u in dataset.user_list], \n",
    "                        dtype=torch.long, device=device)\n",
    "\n",
    "    brand_idx = torch.tensor([dataset.entity_list.index(b) for b in dataset.brand_list], \n",
    "                        dtype=torch.long, device=device)\n",
    "    \n",
    "    # ここもっと上手く書きたい\n",
    "    item_embed = model.entity_embed(item_idx)\n",
    "    item_sim_mat = torch.mm(item_embed, torch.t(item_embed))\n",
    "    item_sim_mat = scipy.sparse.csr_matrix(item_sim_mat.to('cpu').detach().numpy().copy())\n",
    "\n",
    "    user_embed = model.entity_embed(user_idx)\n",
    "    user_sim_mat = torch.mm(user_embed, torch.t(user_embed))\n",
    "    user_sim_mat = scipy.sparse.csr_matrix(user_sim_mat.to('cpu').detach().numpy().copy())\n",
    "\n",
    "    brand_embed = model.entity_embed(brand_idx)\n",
    "    brand_sim_mat = torch.mm(brand_embed, torch.t(brand_embed))\n",
    "    brand_sim_mat = scipy.sparse.csr_matrix(brand_sim_mat.to('cpu').detach().numpy().copy())\n",
    "\n",
    "    M = scipy.sparse.block_diag((item_sim_mat, user_sim_mat, brand_sim_mat))\n",
    "    M_ = np.array(1 - M.sum(axis=1) / np.max(M.sum(axis=1)))\n",
    "                                    \n",
    "    M = M / np.max(M.sum(axis=1)) + scipy.sparse.diags(M_.transpose()[0])\n",
    "    #print(type(M))\n",
    "    #print(M.shape)\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_kg(model):\n",
    "    with torch.no_grad():\n",
    "        batch_size = int(len(dataset.item_list) / 2)\n",
    "        item_index = [dataset.entity_list.index(item) for item in dataset.item_list]\n",
    "        user_index = [dataset.entity_list.index(user) for user in dataset.user_list]\n",
    "        brand_index = [dataset.entity_list.index(brand) for brand in dataset.brand_list]\n",
    "\n",
    "        # user-itemの組に対して予測\n",
    "        u_i_mat = []\n",
    "        for i in user_index:\n",
    "            #pred = torch.tensor([], device=device)\n",
    "            u_i_vec = np.array([])\n",
    "            for j in range(int(len(dataset.item_list) / batch_size) + 1):\n",
    "                # modelにuser,itemを入力\n",
    "                user_tensor = torch.tensor([i for k in range(batch_size)], dtype=torch.long, device=device)\n",
    "                item_tensor = torch.tensor(item_index[j*batch_size : (j+1)*batch_size],\n",
    "                                            dtype=torch.long, device=device)\n",
    "                ### user ->(buy) itemはrelationが0であることに注意 ###\n",
    "                relation_tensor = torch.tensor([0 for k in range(batch_size)], dtype=torch.long, device=device)\n",
    "                \n",
    "                if len(user_tensor) > len(item_tensor):\n",
    "                    user_tensor = torch.tensor([i for k in range(len(item_tensor))],\n",
    "                                            dtype=torch.long, device=device)\n",
    "                    relation_tensor = torch.tensor([0 for k in range(len(item_tensor))],\n",
    "                                                    dtype=torch.long, device=device)\n",
    "\n",
    "                pred = np.array(model.predict(user_tensor, item_tensor, relation_tensor).cpu()) \n",
    "                u_i_vec = np.concatenate([u_i_vec, pred])\n",
    "            u_i_mat.append(u_i_vec)\n",
    "        u_i_mat = np.array(u_i_mat)\n",
    "\n",
    "        # item-itemの組に対して予測\n",
    "        # relationは also_buyとalso_viewの二つ\n",
    "        i_i_b_mat = []\n",
    "        i_i_v_mat = []\n",
    "        for i in item_index:\n",
    "            #pred = torch.tensor([], device=device)\n",
    "            i_i_b_vec = np.array([])\n",
    "            i_i_v_vec = np.array([])\n",
    "            for j in range(int(len(dataset.item_list) / batch_size) + 1):\n",
    "                # modelにuser,itemを入力\n",
    "                h_item_tensor = torch.tensor([i for k in range(batch_size)], dtype=torch.long, device=device)\n",
    "                t_item_tensor = torch.tensor(item_index[j*batch_size : (j+1)*batch_size],\n",
    "                                            dtype=torch.long, device=device)\n",
    "                ### item ->(also_buy) itemはrelationが2であることに注意 ###\n",
    "                ### item ->(also_view) itemはrelationが3であることに注意 ###\n",
    "                b_relation_tensor = torch.tensor([2 for k in range(batch_size)], dtype=torch.long, device=device)\n",
    "                v_relation_tensor = torch.tensor([3 for k in range(batch_size)], dtype=torch.long, device=device)\n",
    "                \n",
    "                if len(h_item_tensor) > len(t_item_tensor):\n",
    "                    h_item_tensor = torch.tensor([i for k in range(len(t_item_tensor))],\n",
    "                                            dtype=torch.long, device=device)\n",
    "                    b_relation_tensor = torch.tensor([2 for k in range(len(t_item_tensor))],\n",
    "                                                    dtype=torch.long, device=device)\n",
    "                    v_relation_tensor = torch.tensor([3 for k in range(len(t_item_tensor))],\n",
    "                                                    dtype=torch.long, device=device)\n",
    "\n",
    "                b_pred = np.array(model.predict(h_item_tensor, t_item_tensor, b_relation_tensor).cpu()) \n",
    "                v_pred = np.array(model.predict(h_item_tensor, t_item_tensor, v_relation_tensor).cpu()) \n",
    "                i_i_b_vec = np.concatenate([i_i_b_vec, b_pred])\n",
    "                i_i_v_vec = np.concatenate([i_i_v_vec, v_pred])\n",
    "            i_i_b_mat.append(i_i_b_vec)\n",
    "            i_i_v_mat.append(i_i_v_vec)\n",
    "        i_i_b_mat = np.array(i_i_b_mat)\n",
    "        i_i_v_mat = np.array(i_i_v_mat)\n",
    "\n",
    "        # item-brandの組に対して予測\n",
    "        i_b_mat = []\n",
    "        for i in item_index:\n",
    "            #pred = torch.tensor([], device=device)\n",
    "            i_b_vec = np.array([])\n",
    "            for j in range(int(len(dataset.item_list) / batch_size) + 1):\n",
    "                # modelにuser,itemを入力\n",
    "                item_tensor = torch.tensor([i for k in range(batch_size)], dtype=torch.long, device=device)\n",
    "                brand_tensor = torch.tensor(brand_index[j*batch_size : (j+1)*batch_size],\n",
    "                                            dtype=torch.long, device=device)\n",
    "                ### item ->(belong) brandはrelationが1であることに注意 ###\n",
    "                relation_tensor = torch.tensor([1 for k in range(batch_size)], dtype=torch.long, device=device)\n",
    "                \n",
    "                if len(item_tensor) > len(brand_tensor):\n",
    "                    item_tensor = torch.tensor([i for k in range(len(brand_tensor))],\n",
    "                                            dtype=torch.long, device=device)\n",
    "                    relation_tensor = torch.tensor([1 for k in range(len(brand_tensor))],\n",
    "                                                    dtype=torch.long, device=device)\n",
    "\n",
    "                if j * batch_size > len(brand_tensor): \n",
    "                    break\n",
    "                pred = np.array(model.predict(item_tensor, brand_tensor, relation_tensor).cpu()) \n",
    "                i_b_vec = np.concatenate([i_b_vec, pred])\n",
    "            i_b_mat.append(i_b_vec)\n",
    "        i_b_mat = np.array(i_b_mat)\n",
    "     \n",
    "    print(u_i_mat.shape)\n",
    "    print(i_i_b_mat.shape)\n",
    "    print(i_i_v_mat.shape)\n",
    "    print(i_b_mat.shape)\n",
    "    u_i_mat = mat_to_graph(user_index, item_index, u_i_mat)\n",
    "    i_i_b_mat = mat_to_graph(item_index, item_index, i_i_b_mat)\n",
    "    i_i_v_mat = mat_to_graph(item_index, item_index, i_i_v_mat)\n",
    "    i_b_mat = mat_to_graph(item_index, brand_index, i_b_mat)\n",
    "\n",
    "    return u_i_mat + i_i_b_mat + i_i_v_mat + i_b_mat\n",
    "\n",
    "\n",
    "def mat_to_graph(row_idx, col_idx, mat):\n",
    "    row_new = []\n",
    "    col_new = []\n",
    "    data = []\n",
    "    for i in range(len(row_idx)):\n",
    "        for j in range(len(col_idx)):\n",
    "            row_new.append(row_idx[i])\n",
    "            col_new.append(col_idx[j])\n",
    "            data.append(mat[i, j])\n",
    "\n",
    "    size = len(dataset.entity_list)\n",
    "    return scipy.sparse.csr_matrix((data, (row_new, col_new)), shape=(size, size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagerank_scipy(G, sim_mat,  personal_vec=None, alpha=0.85, beta=0.01,\n",
    "                   max_iter=500, tol=1.0e-6, weight='weight',\n",
    "                   dangling=None):\n",
    "    \n",
    "    import scipy.sparse\n",
    "\n",
    "    N = len(G)\n",
    "    if N == 0:\n",
    "        return {}\n",
    "\n",
    "    nodelist = G.nodes()\n",
    "    M = nx.to_scipy_sparse_matrix(G, nodelist=nodelist, weight=weight,\n",
    "                                  dtype=float)\n",
    "    S = scipy.array(M.sum(axis=1)).flatten()\n",
    "    S[S != 0] = 1.0 / S[S != 0]\n",
    "    Q = scipy.sparse.spdiags(S.T, 0, *M.shape, format='csr')\n",
    "    M = Q * M\n",
    "\n",
    "    # 遷移行列とsim_matを統合\n",
    "    #sim_mat = mk_sparse_sim_mat(G, item_mat)\n",
    "    M = beta * M + (1 - beta) * sim_mat\n",
    "    \n",
    "    # initial vector\n",
    "    x = scipy.repeat(1.0 / N, N)\n",
    "\n",
    "    \n",
    "    # Personalization vector\n",
    "    p = personal_vec\n",
    " \n",
    "    dangling_weights = p\n",
    "    is_dangling = scipy.where(S == 0)[0]\n",
    "\n",
    "\n",
    "    #print(x.shape)\n",
    "    #print(M.shape)\n",
    "    #print(p.shape)\n",
    "    \n",
    "    ppr_mat = []\n",
    "    for i in range(p.shape[1]):\n",
    "        ppr = power_iterate(N, M, x, p[:, i], dangling_weights[:, i], is_dangling, \n",
    "                            alpha, max_iter, tol)\n",
    "        ppr_mat.append(ppr)\n",
    "        \n",
    "        #if i > 100:\n",
    "        #    print(np.array(ppr_mat).shape)\n",
    "        #    break \n",
    "        \n",
    "    return np.array(ppr_mat)\n",
    "    \n",
    "\n",
    "def power_iterate(N, M, x, p, dangling_weights, is_dangling, alpha, max_iter=500, tol=1.0e-6):\n",
    "    #print(M.shape)\n",
    "    #print(x.shape)\n",
    "    #print(p.shape)\n",
    "    # power iteration: make up to max_iter iterations\n",
    "    for i in range(max_iter):\n",
    "        xlast = x\n",
    "        x = alpha * (x * M + sum(x[is_dangling]) * dangling_weights) + \\\n",
    "            (1 - alpha) * p\n",
    "        # check convergence, l1 norm\n",
    "        x = x / x.sum()\n",
    "        err = scipy.absolute(x - xlast).sum()\n",
    "        if err < N * tol:\n",
    "            #return dict(zip(nodelist, map(float, x)))\n",
    "            #print(i)\n",
    "            return x\n",
    "    # pagerankの収束ちゃんとやっとく\n",
    "    print(x.sum())\n",
    "    print(err)\n",
    "    print(N * tol)\n",
    "    #raise NetworkXError('pagerank_scipy: power iteration failed to converge '\n",
    "                        #'in %d iterations.' % max_iter)\n",
    "        \n",
    "    #return dict(zip(nodelist, map(float, x)))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def pagerank_torch(G, sim_mat, personal_vec, alpha=0.85, beta=0.01,\n",
    "                   max_iter=700, tol=1.0e-6, batch_size=512):\n",
    "\n",
    "    N = len(G)\n",
    "    if N == 0:\n",
    "        return {}\n",
    "\n",
    "    nodelist = G.nodes()\n",
    "    M = nx.to_scipy_sparse_matrix(G, nodelist=nodelist, dtype=float)\n",
    "    S = scipy.array(M.sum(axis=1)).flatten()\n",
    "    S[S != 0] = 1.0 / S[S != 0]\n",
    "    Q = scipy.sparse.spdiags(S.T, 0, *M.shape, format='csr')\n",
    "    M = Q * M\n",
    "    \n",
    "    # 遷移行列とsim_matを統合\n",
    "    M = beta * M + (1 - beta) * sim_mat\n",
    "    M = torch.tensor(M.todense(), dtype=torch.float, device=device).to_sparse() # todenseどうにかしたい\n",
    "    #print(M.shape)\n",
    "    \n",
    "    # Personalization vector\n",
    "    p = torch.tensor(personal_vec, dtype=torch.float, device=device)\n",
    "    #print(p.shape)\n",
    "    \n",
    "    # initial vector\n",
    "    x = torch.ones(N, p.shape[1], dtype=torch.float, device=device) * 1 / N\n",
    "    #print(x.shape)\n",
    "    \n",
    "    # Dangling nodes\n",
    "    dangling_weights = p\n",
    "    is_dangling = scipy.where(S == 0)[0]\n",
    "    #print(is_dangling)\n",
    "    \n",
    "    # power iteration: make up to max_iter iterations\n",
    "    for _ in range(max_iter):\n",
    "        xlast = x\n",
    "        x = alpha * (torch.sparse.mm(M, x) + sum(x[is_dangling]) * dangling_weights) + (1 - alpha) * p\n",
    "        x = x / x.sum(axis=0)\n",
    "        # check convergence, l1 norm\n",
    "        err = torch.abs(x - xlast).sum()\n",
    "        \n",
    "        if err < N * tol * N:\n",
    "            return x\n",
    "        #print(x.shape)\n",
    "        #break\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_ppr(sim_mat, alpha, beta):\n",
    "    \n",
    "    # personal_vecを作る(eneity_size * user_size)\n",
    "    user_idx = [dataset.entity_list.index(u) for u in dataset.user_list]\n",
    "    personal_vec = []\n",
    "    for u in user_idx:\n",
    "        val = np.zeros(len(G.nodes()))\n",
    "        val[u] = 1\n",
    "        personal_vec.append(val[np.newaxis, :])\n",
    "    personal_vec = np.concatenate(personal_vec, axis=0).transpose()\n",
    "    \n",
    "    #ppr = pagerank_torch(G, sim_mat, personal_vec, alpha, beta)\n",
    "    ppr = pagerank_scipy(G, sim_mat, personal_vec, alpha, beta)\n",
    "    \n",
    "    item_idx = [dataset.entity_list.index(i) for i in dataset.item_list]\n",
    "    pred = ppr[:, item_idx]\n",
    "    print(pred.shape)\n",
    "    return pred\n",
    "\n",
    "\n",
    "\n",
    "def get_ranking_mat(model, alpha=0.85, beta=0.01):\n",
    "    ranking_mat = []\n",
    "    sim_mat = mk_sparse_sim_mat(model)\n",
    "    pred = item_ppr(sim_mat, alpha, beta)\n",
    "    #print(pred.shape)\n",
    "    for i in range(len(dataset.user_list)):\n",
    "        sorted_idx = np.argsort(np.array(pred[i]))[::-1]\n",
    "        ranking_mat.append(sorted_idx)\n",
    "        #break\n",
    "    return ranking_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_idx = [dataset.entity_list.index(u) for u in dataset.user_list]\n",
    "user_items_test_dict = pickle.load(open('./data/user_items_test_dict.pickle', 'rb'))\n",
    "\n",
    "def topn_precision(ranking_mat, user_items_dict, n=10):\n",
    "    not_count = 0\n",
    "    precision_sum = 0\n",
    "        \n",
    "    for i in range(len(ranking_mat)):\n",
    "        if len(user_items_dict[user_idx[i]]) == 0:\n",
    "            not_count += 1\n",
    "            continue\n",
    "        sorted_idx = ranking_mat[i]\n",
    "        topn_idx = sorted_idx[:n]  \n",
    "        hit = len(set(topn_idx) & set(user_items_dict[user_idx[i]]))\n",
    "        precision = hit / len(user_items_dict[user_idx[i]])\n",
    "        precision_sum += precision\n",
    "        \n",
    "    return precision_sum / (len(user_idx) - not_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train embed model\n",
    "#model = train_embed(best_params)\n",
    "model = pickle.load(open('model.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def time_since(runtime):\n",
    "    mi = int(runtime / 60)\n",
    "    sec = int(runtime - mi * 60)\n",
    "    return (mi, sec)\n",
    "\n",
    "def objective(trial):\n",
    "    start = time.time()\n",
    "    #gamma = trial.suggest_loguniform('gamma', 1e-6, 1e-3)\n",
    "    #lin_model = trial.suggest_categorical('lin_model', ['lasso', 'elastic'])\n",
    "    #slim = train_SLIM(lin_model, gamma)\n",
    "\n",
    "    alpha = trial.suggest_uniform('alpha', 0, 1)\n",
    "    beta = trial.suggest_uniform('beta', 0, 0.5)\n",
    "    \n",
    "    ranking_mat = get_ranking_mat(model, alpha, beta)\n",
    "    print(ranking_mat[0:5])\n",
    "    score = topn_precision(ranking_mat, user_items_test_dict)\n",
    "    mi, sec = time_since(time.time() - start)\n",
    "    print('{}m{}sec'.format(mi, sec))\n",
    "    \n",
    "    return -1 * score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3819, 1581)\n",
      "[array([1479, 1205,    6, ...,  958,  298,  597]), array([ 79, 766, 507, ..., 108, 877, 527]), array([467, 731, 473, ..., 958, 298, 597]), array([ 298,  922,  597, ..., 1498,   35, 1369]), array([ 504,   93, 1164, ...,  198,  298,  597])]\n",
      "8m58sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-07 11:09:16,824] Finished trial#0 with value: -0.34862567682329587 with parameters: {'alpha': 0.28769269394514074, 'beta': 0.29935157366043297}. Best is trial#0 with value: -0.34862567682329587.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3819, 1581)\n",
      "[array([1479, 1205,    6, ...,  958,  298,  597]), array([ 79, 766, 507, ..., 108, 877, 527]), array([467,  35, 731, ..., 958, 298, 597]), array([ 298,  922,  597, ...,   35, 1369,  467]), array([ 504,   93, 1164, ...,  198,  298,  597])]\n",
      "11m18sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2020-07-07 11:20:35,795] Finished trial#1 with value: -0.33806961785904704 with parameters: {'alpha': 0.4609888561035692, 'beta': 0.47199699171793147}. Best is trial#0 with value: -0.34862567682329587.\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study()\n",
    "study.optimize(objective, n_trials=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = study.trials_dataframe() # pandasのDataFrame形式\n",
    "df.to_csv('./hyparams_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save best params \n",
    "with open('best_param.pickle', 'wb') as f:\n",
    "    pickle.dump(study.best_params, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}