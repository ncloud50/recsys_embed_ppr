{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from networkx.exception import NetworkXError\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "\n",
    "from dataloader import AmazonDataset\n",
    "import models\n",
    "from models import DistMulti, TransE\n",
    "from training import TrainIterater\n",
    "from evaluate import Evaluater\n",
    "\n",
    "import optuna\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'TransE'\n",
    "dataset = AmazonDataset('./data', model_name='TransE')\n",
    "\n",
    "edges = [[r[0], r[1]] for r in dataset.triplet_df.values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KG embedする\n",
    "\n",
    "bestなハイパラパラメータを読み込んでepoch回す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_embed(file_path):\n",
    "    \n",
    "    # ハイパラ読み込み\n",
    "    \n",
    "    \n",
    "    relation_size = len(set(list(dataset.triplet_df['relation'].values)))\n",
    "    entity_size = len(dataset.entity_list)\n",
    "    model = TransE(int(embedding_dim), relation_size, entity_size)\n",
    "    iterater = TrainIterater(batch_size=int(batch_size), model_name=model_name)\n",
    "    score =iterater.iterate_epoch(model, lr=lr, epoch=50, weight_decay=weight_decay, warmup=warmup,\n",
    "                           lr_decay_rate=lr_decay_rate, lr_decay_every=lr_decay_every, eval_every=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習済みembedモデルを読み込む"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# とりあえず初期化したモデルのembeddingを使って進める"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 16\n",
    "relation_size = len(set(list(dataset.triplet_df['relation'].values)))\n",
    "entity_size = len(dataset.entity_list)\n",
    "embed_model = TransE(int(embedding_dim), relation_size, entity_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PageRank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sim_mat(item_len * item_len)を使って隣接行列を作る\n",
    "- 隣接行列((item_len + user_len + brand_len) * (item_len + user_len + brand_len))\n",
    "- nx.google_matrixを参考に隣接行列を作る  \n",
    "\n",
    "\n",
    "danglingあんまり効果がわからないので注意"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph()\n",
    "G.add_nodes_from([i for i in range(len(dataset.entity_list))])\n",
    "G.add_edges_from(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def google_matrix(G, item_mat=None, alpha=0.85, beta=0.01, personalization=None,\n",
    "                  weight='weight', dangling=None):\n",
    "\n",
    "    nodelist = G.nodes()\n",
    "\n",
    "    M = nx.to_numpy_matrix(G, nodelist=nodelist, weight=weight)\n",
    "    N = len(G)\n",
    "    if N == 0:\n",
    "        return M\n",
    "\n",
    "    # Personalization vector\n",
    "    if personalization is None:\n",
    "        p = np.repeat(1.0 / N, N)\n",
    "    else:\n",
    "        missing = set(nodelist) - set(personalization)\n",
    "        if missing:\n",
    "            raise NetworkXError('Personalization vector dictionary '\n",
    "                                'must have a value for every node. '\n",
    "                                'Missing nodes %s' % missing)\n",
    "        p = np.array([personalization[n] for n in nodelist], dtype=float)\n",
    "        p /= p.sum()\n",
    "\n",
    "    #print(p)\n",
    "    #print(p.shape)\n",
    "        \n",
    "    # Dangling nodes\n",
    "    if dangling is None:\n",
    "        dangling_weights = p\n",
    "    else:\n",
    "        missing = set(nodelist) - set(dangling)\n",
    "        if missing:\n",
    "            raise NetworkXError('Dangling node dictionary '\n",
    "                                'must have a value for every node. '\n",
    "                                'Missing nodes %s' % missing)\n",
    "        # Convert the dangling dictionary into an array in nodelist order\n",
    "        dangling_weights = np.array([dangling[n] for n in nodelist],\n",
    "                                    dtype=float)\n",
    "        dangling_weights /= dangling_weights.sum()\n",
    "    dangling_nodes = np.where(M.sum(axis=1) == 0)[0]\n",
    "\n",
    "    # Assign dangling_weights to any dangling nodes (nodes with no out links)\n",
    "    for node in dangling_nodes:\n",
    "        M[node] = dangling_weights\n",
    "\n",
    "    \n",
    "    M /= M.sum(axis=1)  # Normalize rows to sum to 1\n",
    "    \n",
    "    if item_mat is not None:\n",
    "        sim_mat = mk_sim_mat(G, item_mat)\n",
    "\n",
    "        M = beta * M + (1 - beta) * sim_mat\n",
    "    \n",
    "    return alpha * M + (1 - alpha) * p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_sim_mat(G, item_mat):\n",
    "    M = np.eye(len(G.nodes()))\n",
    "    #M = np.eye(4)\n",
    "    item_len = item_mat.shape[0]\n",
    "    M[0:item_len, 0:item_len] = item_mat\n",
    "    \n",
    "    # RecWalk論文の定義\n",
    "    M = M / np.max(M.sum(axis=1)) + np.diag(1 - M.sum(axis=1) / np.max(M.sum(axis=1)))\n",
    "   \n",
    "    return M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sparse sim_matを作る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AmazonDataset('./data', model_name='TransE')\n",
    "\n",
    "def mk_sparse_sim_mat(model):\n",
    "    item_idx = torch.tensor([dataset.entity_list.index(i) for i in dataset.item_list], \n",
    "                        dtype=torch.long, device=device)\n",
    "\n",
    "    user_idx = torch.tensor([dataset.entity_list.index(u) for u in dataset.user_list], \n",
    "                        dtype=torch.long, device=device)\n",
    "\n",
    "    brand_idx = torch.tensor([dataset.entity_list.index(b) for b in dataset.brand_list], \n",
    "                        dtype=torch.long, device=device)\n",
    "    \n",
    "    # ここもっと上手く書きたい\n",
    "    item_embed = model.entity_embed(item_idx)\n",
    "    item_sim_mat = torch.mm(item_embed, torch.t(item_embed))\n",
    "    item_sim_mat = scipy.sparse.csr_matrix(item_sim_mat.to('cpu').detach().numpy().copy())\n",
    "\n",
    "    user_embed = model.entity_embed(user_idx)\n",
    "    user_sim_mat = torch.mm(user_embed, torch.t(user_embed))\n",
    "    user_sim_mat = scipy.sparse.csr_matrix(user_sim_mat.to('cpu').detach().numpy().copy())\n",
    "\n",
    "    brand_embed = model.entity_embed(brand_idx)\n",
    "    brand_sim_mat = torch.mm(brand_embed, torch.t(brand_embed))\n",
    "    brand_sim_mat = scipy.sparse.csr_matrix(brand_sim_mat.to('cpu').detach().numpy().copy())\n",
    "\n",
    "    M = scipy.sparse.block_diag((item_sim_mat, user_sim_mat, brand_sim_mat))\n",
    "    M_ = np.array(1 - M.sum(axis=1) / np.max(M.sum(axis=1)))\n",
    "                                    \n",
    "    M = M / np.max(M.sum(axis=1)) + scipy.sparse.diags(M_.transpose()[0])\n",
    "    #print(type(M))\n",
    "    #print(M.shape)\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5407x5407 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 17084371 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mk_sparse_sim_mat(embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def pagerank_numpy(G, item_mat, alpha=0.85, beta=0.01, personalization=None, weight='weight',\n",
    "                   dangling=None):\n",
    "\n",
    "    import numpy as np\n",
    "    if len(G) == 0:\n",
    "        return {}\n",
    "    M = google_matrix(G, item_mat, alpha, beta, personalization=personalization,\n",
    "                      weight=weight, dangling=dangling)\n",
    "    #return 0\n",
    "    # use numpy LAPACK solver\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(M.T)\n",
    "    ind = eigenvalues.argsort()\n",
    "    # eigenvector of largest eigenvalue at ind[-1], normalized\n",
    "    largest = np.array(eigenvectors[:, ind[-1]]).flatten().real\n",
    "    norm = float(largest.sum())\n",
    "    return dict(zip(G, map(float, largest / norm)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagerank_scipy(G, model, alpha=0.85, beta=0.01, personalization=None,\n",
    "                   max_iter=100, tol=1.0e-6, weight='weight',\n",
    "                   dangling=None):\n",
    "    \n",
    "    import scipy.sparse\n",
    "\n",
    "    N = len(G)\n",
    "    if N == 0:\n",
    "        return {}\n",
    "\n",
    "    nodelist = G.nodes()\n",
    "    M = nx.to_scipy_sparse_matrix(G, nodelist=nodelist, weight=weight,\n",
    "                                  dtype=float)\n",
    "    S = scipy.array(M.sum(axis=1)).flatten()\n",
    "    S[S != 0] = 1.0 / S[S != 0]\n",
    "    Q = scipy.sparse.spdiags(S.T, 0, *M.shape, format='csr')\n",
    "    M = Q * M\n",
    "\n",
    "    # initial vector\n",
    "    x = scipy.repeat(1.0 / N, N)\n",
    "\n",
    "    # Personalization vector\n",
    "    if personalization is None:\n",
    "        p = scipy.repeat(1.0 / N, N)\n",
    "    else:\n",
    "        missing = set(nodelist) - set(personalization)\n",
    "        if missing:\n",
    "            raise NetworkXError('Personalization vector dictionary '\n",
    "                                'must have a value for every node. '\n",
    "                                'Missing nodes %s' % missing)\n",
    "        p = scipy.array([personalization[n] for n in nodelist],\n",
    "                        dtype=float)\n",
    "        p = p / p.sum()\n",
    "\n",
    "    # Dangling nodes\n",
    "    if dangling is None:\n",
    "        dangling_weights = p\n",
    "    else:\n",
    "        missing = set(nodelist) - set(dangling)\n",
    "        if missing:\n",
    "            raise NetworkXError('Dangling node dictionary '\n",
    "                                'must have a value for every node. '\n",
    "                                'Missing nodes %s' % missing)\n",
    "        # Convert the dangling dictionary into an array in nodelist order\n",
    "        dangling_weights = scipy.array([dangling[n] for n in nodelist],\n",
    "                                       dtype=float)\n",
    "        dangling_weights /= dangling_weights.sum()\n",
    "    is_dangling = scipy.where(S == 0)[0]\n",
    "\n",
    "    \n",
    "    # 遷移行列とsim_matを統合\n",
    "    sim_mat = mk_sparse_sim_mat(model)\n",
    "    M = beta * M + (1 - beta) * sim_mat\n",
    "\n",
    "    #return 0\n",
    "    # power iteration: make up to max_iter iterations\n",
    "    for _ in range(max_iter):\n",
    "        xlast = x\n",
    "        x = alpha * (x * M + sum(x[is_dangling]) * dangling_weights) + \\\n",
    "            (1 - alpha) * p\n",
    "        # check convergence, l1 norm\n",
    "        err = scipy.absolute(x - xlast).sum()\n",
    "        if err < N * tol:\n",
    "            return dict(zip(nodelist, map(float, x)))\n",
    "    raise NetworkXError('pagerank_scipy: power iteration failed to converge '\n",
    "                        'in %d iterations.' % max_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val = np.zeros(len(G.nodes()))\n",
    "val[user_idx[10]] = 1\n",
    "k = [i for i in range(len(G.nodes()))]\n",
    "personal_vec = dict(zip(k, val))\n",
    "pagerank_scipy(G, model, alpha = 0.9, beta=0.01, personalization=personal_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_idx = [entity_list.index(u) for u in user_list]\n",
    "\n",
    "def item_ppr(slim, user, alpha, beta):\n",
    "    val = np.zeros(len(G.nodes()))\n",
    "    val[user] = 1\n",
    "    k = [i for i in range(len(G.nodes()))]\n",
    "    personal_vec = dict(zip(k, val))\n",
    "    #print(personal_vec)\n",
    "    #pr = pagerank_numpy(G, slim.sim_mat, alpha, beta, personalization=personal)\n",
    "    #pr = pagerank_scipy(G, slim.sim_mat, alpha, beta, personalization=personal)\n",
    "    #return pr\n",
    "    \n",
    "    # random 後で消す\n",
    "    # val = np.random.dirichlet([1 for i in range(len(G.nodes))], 1)[0]\n",
    "    val = np.random.rand(len(G.nodes()))\n",
    "    val /= val.sum()\n",
    "    k = [i for i in range(len(G.nodes))]\n",
    "    ppr = dict(zip(k, val))\n",
    "    \n",
    "    pred = []\n",
    "    item_idx = [entity_list.index(i) for i in item_list]\n",
    "    for i in item_idx:\n",
    "        pred.append(ppr[i])\n",
    "    \n",
    "    return pred\n",
    "\n",
    "\n",
    "def get_ranking_mat(slim, alpha=0.85, beta=0.01):\n",
    "    ranking_mat = []\n",
    "    count = 0\n",
    "    for u in user_idx:\n",
    "        pred = item_ppr(slim, u, alpha, beta)\n",
    "        #print(pred)\n",
    "        sorted_idx = np.argsort(np.array(pred))[::-1]\n",
    "        ranking_mat.append(sorted_idx)\n",
    "        \n",
    "        #count += 1\n",
    "        #if count > 100:\n",
    "        #    break\n",
    "            \n",
    "    return ranking_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_idx = [entity_list.index(u) for u in user_list]\n",
    "user_items_test_dict = pickle.load(open('./data/user_items_test_dict.pickle', 'rb'))\n",
    "\n",
    "def topn_precision(ranking_mat, user_items_dict, n=10):\n",
    "    not_count = 0\n",
    "    precision_sum = 0\n",
    "        \n",
    "    for i in range(len(ranking_mat)):\n",
    "        if len(user_items_dict[user_idx[i]]) == 0:\n",
    "            not_count += 1\n",
    "            continue\n",
    "        sorted_idx = ranking_mat[i]\n",
    "        topn_idx = sorted_idx[:n]  \n",
    "        hit = len(set(topn_idx) & set(user_items_dict[user_idx[i]]))\n",
    "        precision = hit / len(user_items_dict[user_idx[i]])\n",
    "        precision_sum += precision\n",
    "        \n",
    "    return precision_sum / (len(user_idx) - not_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    gamma = 1e-4\n",
    "    lin_model = 'lasso'\n",
    "    slim = train_SLIM(lin_model, gamma)\n",
    "    \n",
    "    alpha = 0.85\n",
    "    beta = 0.01\n",
    "    \n",
    "    ranking_mat = get_ranking_mat(slim, alpha, beta)\n",
    "    score = topn_precision(ranking_mat, user_items_test_dict)\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    gamma = trial.suggest_loguniform('gamma', 1e-6, 1e-3)\n",
    "    lin_model = trial.suggest_categorical('lin_model', ['lasso', 'elastic'])\n",
    "    slim = train_SLIM(lin_model, gamma)\n",
    "    \n",
    "    alpha = trial.suggest_uniform('alpha', 0, 1)\n",
    "    beta = trial.suggest_uniform('beta', 0, 0.5)\n",
    "    \n",
    "    ranking_mat = get_ranking_mat(slim, alpha, beta)\n",
    "    score = topn_precision(ranking_mat, user_items_test_dict)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study()\n",
    "study.optimize(objective, n_trials=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005572710458254794\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
